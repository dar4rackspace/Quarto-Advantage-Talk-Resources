[
  {
    "objectID": "onboarding docs/first_week_onboarding.html",
    "href": "onboarding docs/first_week_onboarding.html",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#logging-in",
    "href": "onboarding docs/first_week_onboarding.html#logging-in",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#computer-setup",
    "href": "onboarding docs/first_week_onboarding.html#computer-setup",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Computer setup",
    "text": "Computer setup\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#vpn-setup",
    "href": "onboarding docs/first_week_onboarding.html#vpn-setup",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "VPN Setup",
    "text": "VPN Setup\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#sailpoint-access-request",
    "href": "onboarding docs/first_week_onboarding.html#sailpoint-access-request",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Sailpoint access request",
    "text": "Sailpoint access request\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#gcp-bigquery-setup",
    "href": "onboarding docs/first_week_onboarding.html#gcp-bigquery-setup",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "GCP (BigQuery) Setup",
    "text": "GCP (BigQuery) Setup\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#timesheets",
    "href": "onboarding docs/first_week_onboarding.html#timesheets",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Timesheets",
    "text": "Timesheets\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#meetings-to-be-aware-of",
    "href": "onboarding docs/first_week_onboarding.html#meetings-to-be-aware-of",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Meetings to be aware of",
    "text": "Meetings to be aware of\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#innovation-day",
    "href": "onboarding docs/first_week_onboarding.html#innovation-day",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Innovation Day",
    "text": "Innovation Day\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#communication-tools",
    "href": "onboarding docs/first_week_onboarding.html#communication-tools",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Communication Tools",
    "text": "Communication Tools\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#data-intake-process",
    "href": "onboarding docs/first_week_onboarding.html#data-intake-process",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Data Intake Process",
    "text": "Data Intake Process\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "onboarding docs/first_week_onboarding.html#important-links",
    "href": "onboarding docs/first_week_onboarding.html#important-links",
    "title": "Onboarding Guide for Reporting & Analytics Team",
    "section": "Important Links",
    "text": "Important Links\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed fringilla, nulla non posuere mollis, erat augue ultricies lectus, vel feugiat est sapien eget nulla. Vestibulum efficitur, velit id lacinia tristique, ipsum metus aliquam risus, nec hendrerit ligula lorem a risus. Nullam ornare, eros sit amet hendrerit efficitur, dui turpis consequat enim, non congue est libero eu ipsum.",
    "crumbs": [
      "First Week @Rackspace",
      "Onboarding Guide for Reporting & Analytics Team"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BI Enterprise Docs",
    "section": "",
    "text": "This is an example documentation website based off Quarto"
  },
  {
    "objectID": "index.html#example-website",
    "href": "index.html#example-website",
    "title": "BI Enterprise Docs",
    "section": "",
    "text": "This is an example documentation website based off Quarto"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Example about page"
  },
  {
    "objectID": "about.html#example",
    "href": "about.html#example",
    "title": "About",
    "section": "",
    "text": "Example about page"
  },
  {
    "objectID": "bi-infrastructure.html",
    "href": "bi-infrastructure.html",
    "title": "B.I Enterprise Infrastructure",
    "section": "",
    "text": "Big Query is used as the landing place for most datasources @ Rackspace in the form of ods (original datasource). Then that information is used, replicated and transformed by different Google Projects across the organization. Below is a rough sketch of the databases identified that point towards Big Query:\n\n\n\n\n\n---\ntitle: \"ODS identified that have a dataset in BigQuery\"\n---\n\ngraph TD;\n\n BigQuery[(BigQuery)]\n\nsubgraph \"Customer Management\"\n    Datasource1[(Salesforce)] \n    Datasource4[(Inhouse CMS)]\n    Datasource6[(Encore)] \nend\n\nDatasource1[(Salesforce)] --&gt; |salesforce_ods| BigQuery;\n    Datasource4[(Inhouse CMS)] --&gt; |cms_ods| BigQuery;\n    Datasource6[(Encore)] --&gt; |encore_ticketing_ods|BigQuery;\n    \nsubgraph \"Billing\"\n    Datasource2[(Oracle BRM)] \nend\n\nDatasource2[(Oracle BRM)] --&gt; |brm_ods| BigQuery;\n\nsubgraph \"IT Services\"\n    Datasource5[(Core)] \n    Datasource7[(VMWare)]\n    Datasource8[(SCCM)] \n    Datasource9[(Microsoft365)]\nend\n\nsubgraph \"B.I Internal\"\n    Datasource10[(Jira)] \n    Datasource11[(Tableau)]\n    Datasource12[(PowerBI)] \nend\n\n Datasource5[(Core)] --&gt; |core_ods| BigQuery;\n    Datasource7[(VMWare)] --&gt; |double check| BigQuery;\n    Datasource8[(SCCM)] --&gt; |PENDING: system center configuration manager| BigQuery;\n    Datasource10 --&gt; |jira_ods| BigQuery"
  },
  {
    "objectID": "bi-infrastructure.html#the-datalake",
    "href": "bi-infrastructure.html#the-datalake",
    "title": "B.I Enterprise Infrastructure",
    "section": "",
    "text": "Big Query is used as the landing place for most datasources @ Rackspace in the form of ods (original datasource). Then that information is used, replicated and transformed by different Google Projects across the organization. Below is a rough sketch of the databases identified that point towards Big Query:\n\n\n\n\n\n---\ntitle: \"ODS identified that have a dataset in BigQuery\"\n---\n\ngraph TD;\n\n BigQuery[(BigQuery)]\n\nsubgraph \"Customer Management\"\n    Datasource1[(Salesforce)] \n    Datasource4[(Inhouse CMS)]\n    Datasource6[(Encore)] \nend\n\nDatasource1[(Salesforce)] --&gt; |salesforce_ods| BigQuery;\n    Datasource4[(Inhouse CMS)] --&gt; |cms_ods| BigQuery;\n    Datasource6[(Encore)] --&gt; |encore_ticketing_ods|BigQuery;\n    \nsubgraph \"Billing\"\n    Datasource2[(Oracle BRM)] \nend\n\nDatasource2[(Oracle BRM)] --&gt; |brm_ods| BigQuery;\n\nsubgraph \"IT Services\"\n    Datasource5[(Core)] \n    Datasource7[(VMWare)]\n    Datasource8[(SCCM)] \n    Datasource9[(Microsoft365)]\nend\n\nsubgraph \"B.I Internal\"\n    Datasource10[(Jira)] \n    Datasource11[(Tableau)]\n    Datasource12[(PowerBI)] \nend\n\n Datasource5[(Core)] --&gt; |core_ods| BigQuery;\n    Datasource7[(VMWare)] --&gt; |double check| BigQuery;\n    Datasource8[(SCCM)] --&gt; |PENDING: system center configuration manager| BigQuery;\n    Datasource10 --&gt; |jira_ods| BigQuery"
  },
  {
    "objectID": "bi-infrastructure.html#the-b.i-tools",
    "href": "bi-infrastructure.html#the-b.i-tools",
    "title": "B.I Enterprise Infrastructure",
    "section": "The B.I Tools",
    "text": "The B.I Tools\nFor now tableau the architecture is as follows:\nA dashboard in tableau is a Workbook with a collection of 1 or more Views. A Workbook needs at least 1 Datasource to connect to. Most connections we have are with BigQuery but its possible to have others.\nFor compliance and governability is best advise to seek to land all information in BigQuery, and from there connect to tableau.\n\n\n\n\n\n\n\n\ngraph TD;\n\nsubgraph \"Tableau Environment\"\nsubgraph \"Tableau Dashboard\"\n    Views --&gt; Workbooks;\nend\n\n    Workbooks --&gt; Datasources;\n\nsubgraph \"Datasources\"\n    BQTables;\n    GoogleSheets;\n    ODS;\nend\n\n    Datasources --&gt; |BigQuery Tables| BQTables;\n    Datasources --&gt; |Google Sheets| GoogleSheets;\n    Datasources --&gt; |Operational Data Stores| ODS;\n\nend\n\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "onboarding docs/bi_sql_development_standards.html",
    "href": "onboarding docs/bi_sql_development_standards.html",
    "title": "BI SQL Development Standards",
    "section": "",
    "text": "This Document contained BI development standards…..",
    "crumbs": [
      "Our Best Practices",
      "BI SQL Development Standards"
    ]
  },
  {
    "objectID": "onboarding docs/bi_sql_development_standards.html#dataform-standards",
    "href": "onboarding docs/bi_sql_development_standards.html#dataform-standards",
    "title": "BI SQL Development Standards",
    "section": "Dataform Standards",
    "text": "Dataform Standards\nThe exact same as the BigQuery standards apply to Dataform, with the following exceptions:\n\nA single sqlx script generates a single table / view\nAll tables created in Dataform go to rax-enterprisebi.dataform.&lt;table_name&gt; (production) or rax-enterprisebi-dev.dataform.&lt;table_name&gt; (development)\nThere are no prefixes nor suffixes to sqlx scripts. The only rule is &lt;report_name&gt;.sqlx or &lt;report_name&gt;_&lt;table_name&gt;.sqlx (if more than one table)\nSingle table in report: report_specific/account_alignment/account_alignment.sqlx\nOne of multiple tables in same report: report_specific/pull_account/pull_account_contact_info.sqlx\nTable creation, documentation, clustering, etc. happen in the config object\nVariable declarations happen in the pre_operations object\nOperation sqlx scripts are used to do anything outside table creation - such as exporting data to GCS",
    "crumbs": [
      "Our Best Practices",
      "BI SQL Development Standards"
    ]
  },
  {
    "objectID": "onboarding docs/bi_sql_development_standards.html#bigquery-standards",
    "href": "onboarding docs/bi_sql_development_standards.html#bigquery-standards",
    "title": "BI SQL Development Standards",
    "section": "BigQuery Standards",
    "text": "BigQuery Standards\n\nGeneral Coding Practices\n\nTables\n\nDo not use nesting for subqueries, unless they’re extremely simple. Use CTEs instead (or temporary tables if CTEs are not an option)\nThree-part naming standards must be followed when referring to datasets (e.g. rax-landing-dev.cms_ods.customer_account)\nWhen querying more than one table:\n\nAll tables should be aliased with 1-3 characters (Iines 31, 32, 35)\nAll field names should be preceded by the table alias in the query (l. 17-30)\nAll tables and field names must be snake_case (lowercase letters, with words separated by underscores)\nTables and column names must not set reserved words (i.e. function names)\nFields should be aliased with AS instead of using the = keyword (l. 7)\n\n\n\n\nComments\n\nCode should have sections and in line comments indicating what is happening:\n\nSections should use multi-line comments (lines 1-3)\nSpecific purpose of the code can be documented with one-line comments (l. 4)\n\n\n\n\nLine format\n\nEach new field should be carried down to its own line.\nCommas should be placed at the end of the line, and not at the beginning.\nCase statements should be structured and indented as shown (l. 19-23).\n\n\n\nKey Words\n\nAll keywords (SELECT, FROM, JOIN, WHERE, GROUP BY, HAVING, INTO, CASE, SUM, COUNT, etc.) should be all caps.\nThe GROUP BY statement should be written with field names all on the same line.\n\n\n\nVariables\n\nUse declared variables when programming hard-coded dates (ie: min / max dates) or conditions that are repeated.\nVariable names must always start with v_. This is done in order to make it easier to find variables within the script (e.g.: v_min_date).\n\n\n\nNested queries (don’t)\n\nNested queries (sub selects) should be avoided, and temp tables or common table expressions should be used instead. This makes code easier to read and maintain.\n\n\n\nOperators\n\nFor mostly all operators such as =, +, &gt;=, &lt;=, &gt;, &lt;, there needs to be only one space before and after each operator:\n\nCorrect Formatting - Operators\n\nTHIS:\n\nfoo + bar\n\nNOT THIS:\n\nfoo+ bar\nfoo +bar\nfoo+bar\n\n\nThe only exceptions are parentheses, quotes and brackets:\n\nCorrect Formatting - Operators\n\nTHIS:\n\nCAST(foo AS STRING)\n\nNOT THIS:\n\nCAST( foo AS STRING )\nCAST(foo AS STRING )\nCAST( foo AS STRING)\n\n\n\n\n\n\n\nComments\n\nCode is thoroughly commented so any developer can understand what it is doing when necessary. The proper way to comment is shown below.\n\nProper Commenting On Qlik Code (Multiline)\n\n\n    /*********************************************************************************************\n    Insert comment here\n    This is a multiline comment - best used at the start of sections\n    *********************************************************************************************/\n    SELECT foo\n    FROM bar # This is an in-line comment\n\n\nIndentation\n\nCode is consistently indented properly to make it readable. This means it can follow any of the below examples (but not mix their rules)\n\nAlingment examples\n\n– Format A: Indent by alighning on field name\n\nSELECT foo, bar, CASE WHEN foo = 1 THEN foo WHEN foo = 0 THEN no_foo ELSE NULL END AS foo_flag FROM foo_bar_table;\n\n– Format B: Indent on tab (4 spaces)\n\nSELECT DISTINCT foo, bar, CASE WHEN foo = 1 THEN foo WHEN foo = 0 THEN no_foo ELSE NULL END AS foo_flag FROM foo_bar_table;\n\n\nRegardless of which indentation rules are used, tabs are converted to spaces to avoid GitHub messing up file readability (for more on this, read What’s the deal with tab sizes on GitHub Gists? - GitHub Help / How to use Git and GitHub - GitHub Community)\nThis changes nothing in the script itself, except the way in which the IDE processes indentation, so your code should look the same.\nYou can set this once and forget it in any IDE, usually through a menu or the command palette (SHIFT+CTRL+P).\nHere’s how to do it for the more popular IDEs: Notepad ++ | VSCode | Sublime | Atom\nFor an example of how tabs affect document formatting, look at how the dots (spaces) and arrows (tabs) look in the IDE (left) vs GitHub (right):",
    "crumbs": [
      "Our Best Practices",
      "BI SQL Development Standards"
    ]
  },
  {
    "objectID": "onboarding docs/bi_sql_development_standards.html#optimization-guidelines-for-bi-report-building",
    "href": "onboarding docs/bi_sql_development_standards.html#optimization-guidelines-for-bi-report-building",
    "title": "BI SQL Development Standards",
    "section": "Optimization Guidelines for BI Report Building",
    "text": "Optimization Guidelines for BI Report Building\n\nLIMIT N does not reduce query cost.\nDo not use SELECT * unless you need all the columns in a table. Each column queried increases cost.\n\n“But I need 19 out of 20 fields and writing them all is just wasted space…” - use SELECT * EXCEPT(field_name).\nSELECT * EXCEPT() must not be used to pull data from a source table (i.e. a table outside the scope of the BI report that is used as the source of data).\n\nUse the partitioning and clustering fields in WHERE and JOIN clauses to reduce costs.\nUse UNION ALL instead of UNION DISTINCT where possible.\nDo not use complicated calculated fields in JOINS or GROUP BY clauses.\n\nCreate the key fields beforehand in temporary tables.\n\nDo not use OR in JOIN statements. This is extremely slow.\n\nSeparate your table into smaller pieces, do the separate JOINS and then UNION the new tables instead.\n\nDo not use views to query other views, unless these views are from a different report, dataset, or project.",
    "crumbs": [
      "Our Best Practices",
      "BI SQL Development Standards"
    ]
  },
  {
    "objectID": "onboarding docs/bi_sql_development_standards.html#table-view-creation-standards",
    "href": "onboarding docs/bi_sql_development_standards.html#table-view-creation-standards",
    "title": "BI SQL Development Standards",
    "section": "Table / View Creation Standards",
    "text": "Table / View Creation Standards\n\nStored Procedure Scripts\n\nNames for stored procedures all start with udsp_ and end in either _full or _incremental, depending on the load type.\n\nFull means the entire table is truncated and all data is inserted from scratch, every time it reloads.\nIncrement al means only the latest part of the table is truncated and replaced by new data. This is more efficient, but also harder to code.\n\nThe entire stored procedure should be contained in a single script.\nStored procedures must NOT create tables. They must instead truncate / delete and insert data to tables created by table creation scripts.\nAll stored procedures must include error handling.\nAll stored procedures must invoke the logging for load dependencies.\nThere are specific folders for report-specific and general-reporting scripts in the GitHub repo.\n\nStored Proc Boilerplate \nCREATE OR REPLACE PROCEDURE `rax-enterprisebi.report_specific / report_table.TABLE NAME`()\nBEGIN\n    DECLARE log_start TIMESTAMP DEFAULT CURRENT_TIMESTAMP(); # This is used for table logging\n    BEGIN\n        CALL `rax-enterprisebi.load_dependencies.udsp_table_load_log`('report_specific / report table', 'Table name', 'SP Name', log_start, NULL, NULL); # Invoke table logging\n        ------\n        /*************************************************************************************\n        Section notes go here\n        *************************************************************************************/\n        &lt;CTE or Temp table creation code goes here&gt;\n\n        # If above query was successful, truncate table\n        TRUNCATE TABLE `TABLE NAME`;\n\n        # Insert new data into table\n        INSERT INTO `TABLE NAME` \n        SELECT * \n        FROM `TEMP TABLE NAME`;\n\n        # Call table log as completed\n        CALL`rax-enterprisebi.load_dependencies.udsp_table_load_log`('report_specific / report table', 'Table name', 'SP Name', log_start, CURRENT_TIMESTAMP(), NULL);\n\n    # Call table log as failed, including error message\n    EXCEPTION WHEN ERROR THEN   \n        CALL`rax-enterprisebi.load_dependencies.udsp_table_load_log`('report_specific / report table', 'Table name', 'SP Name', log_start, CURRENT_TIMESTAMP(), @@error.message);\n\n        # Restore table back to original to ensure it is not empty\n        INSERT INTO `TABLE NAME` \n        SELECT * FROM `TEMP TABLE NAME` \n        FOR SYSTEM_TIME AS OF log_start\n    END;\nEND\n\nTable Creation Scripts (for Stored Procedures)\n\nPartitioning and clustering should be added to final tables where it makes sense.\n\nKeep in mind there is a limit of 4000 unique partition values on a table (~10 years, or way more if partitioning by month).\nTo partition by month, use a truncated date field. This avoids confusion and allows for better data handling in Tableau.\n\nIf incremental loading will be used for the table, add a partition expiration.\nThe name for the table creation script should be the name of the table.\nAn individual table creation script is needed for every table created (this should be changed) ??????????????????????\n\n\n\nTable Creation Example\nCREATE OR REPLACE TABLE `rax-enterprisebi.report_specific.account360_account_data`(\n account_number STRING,\n account_type STRING,\n account_id STRING,\n company_name STRING,\n company_name_filter_a STRING,\n company_name_filter_b STRING,\n top_200_salesforce BOOLEAN,\n naics_description STRING,\n discoverorgid STRING,\n parent_id STRING,\n account_identifier STRING,\n date_partition DATE #Need to partition table in order to cluster\n)\nPARTITION BY DATE_TRUNC(date_partition, MONTH)\nCLUSTER BY account_type, account_number, account_identifier, account_id;\n\n\n\nView Creation Scripts\n\nViews should follow the same naming convention as other tables (we do not prefix with vw_ now, as table functions can change)\nAll views for a specific workbook / report should be contained within a single script.\nViews should only be used at the very end of the data flow, as inputs for BI reports, such as Tableau workbooks, for example.\n\n\n\nWhen Should I Use a Stored Proc and When Should I Use a View?\n\nUse a stored procedure when:\n\nThe operation is data intensive and creates big tables (query times greater than a minute in BQ).\nTable logging is required.\nThe output will be used by more than one BI report.\nA custom function or variables need to be defined.\n\nUse a view when:\n\nThere is a clear need for the data to be calculated at the time of the query.\nThere are only small changes needed to an existing general-use table, like new field calculations.\n\n\n\n\n\n\n\n\nDO NOT USE A VIEW\n\n\n\n\nTo query another view within the scope of the same BI report. This is expensive, inefficient, and hard to maintain.\nEither query the source table behind the view or refactor your script.\n\n\n\n\nTo change data types for existing tables.\n\nData types can be changed by refactoring the source table, or inside the Tableau Data Source.\nViews that are used for multiple workbooks should be turned into stored procedures that create general-reporting tables. This reduces query time and costs:\n\nIMAGEEEEEEEEEEEEEEEEE",
    "crumbs": [
      "Our Best Practices",
      "BI SQL Development Standards"
    ]
  },
  {
    "objectID": "onboarding docs/bi_sql_development_standards.html#github",
    "href": "onboarding docs/bi_sql_development_standards.html#github",
    "title": "BI SQL Development Standards",
    "section": "GitHub",
    "text": "GitHub\n\nWe use GitHub as our Version Control System (VCS).\nOur GitHub workflow is based off of Understanding the GitHub Flow\nYou can work with GitHub through the UI or using git command line, whichever is more comfortable for you\nTeam walkthrough video can be found here with passcode 33n*$gWg DEPRECATED!\n\n\nGitHub Access\n\nTo develop tables and stored procedures for the rax-enterprisebi project, you will need to request access to rax-gdo-bideveloper through SailPoint.\nIf not developing tables and stored procedures, please work with the Ops team to determine the proper group for access.\n\n\n\nBranches:\n\nMain will reflect the code that is running on production\n\nNo development work should be done on the main branch\nAll code on the main branch has been reviewed\n\nA new branch is created for each feature being worked\n\nAfter a pull request/review is completed for the feature branch, the items can be merged with main and the branch removed",
    "crumbs": [
      "Our Best Practices",
      "BI SQL Development Standards"
    ]
  },
  {
    "objectID": "onboarding docs/bi_sql_development_standards.html#bigquery-vcs",
    "href": "onboarding docs/bi_sql_development_standards.html#bigquery-vcs",
    "title": "BI SQL Development Standards",
    "section": "BigQuery VCS",
    "text": "BigQuery VCS\n\nAll GCP table and stored procedure code is stored in our entreprise GitHub repository\nEdit: most of these tables will eventually be migrated into the new Dataform repository, which allows for CI/CD with our BI code: rax-bivis/dataform\n\n\nAccess\n\nRequest access to github-cloud-users through SailPoint\nCreate a public GitHub account\nYou must use your full name and Rackspace email\nYour username must be your SSO\nTwo-factor authentication must be enabled\nAdd your GitHub username to the identity portal\nRequest access to Global Data Office by submitting a JIRA request to GDO\n\nExample ticket\n\nFolders:\n\nThe folders in GitHub will be the same as the datasets in GCP and there will be a separate folder for tables and stored procedures\nYour create table statements will go into the tables folder\nThe stored procedures used to populate the table will go in the stored procedures folder\nYou may have more than one stored procedure per table (one that is full and a second that is incremental, if applicable)\n\n\n\n\nProcess Details:\n\nClone the main branch to your computer git clone and git pull\nCreate a new branch for you feature/development git branch\n\nAdd new .sql files with your BigQuery code OR edit existing files\n\nAdd your files to your branch git add and commit them git commit and when you are ready for review git push\nCreate a pull request and fill in the Developer items on the pull request template (to make this easiest, this entire step should be done through the UI on GitHub’s website)\n\nAsk someone to review (reviews will also be assigned in the bi-weekly peer review meetings)\nReviewer reviews code and comments on any changes that are needed\n\nReviewer must check all boxes in the pull request template\nReviewer should only approve pull request when it is deemed ready for production\n\n\nAfter pull request is approved, it can be merged with main, deployed to production (rax-enterprisebi), and the development/feature branch can be removed\nDeployment is a manual process right now and will be done by the Developer",
    "crumbs": [
      "Our Best Practices",
      "BI SQL Development Standards"
    ]
  }
]